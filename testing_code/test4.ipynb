{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (0.27.0)\n",
      "Requirement already satisfied: filelock in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jasper/Downloads/LangChain-AI-Engineer-Workshop/.conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('/Users/jasper/Downloads/My_first_agents-main/notebooks/.env')\n",
    "\n",
    "# Verify that the API key is loaded\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! What is happiness?\n",
      "\n",
      "JACK\n",
      "Happiness is a butterfly, blooming in the morning\n",
      "And holding its wings out like an open hand.\n",
      "It's the joy that comes when life is new\n",
      "And the knowledge that tomorrow will be better.\n",
      "\n",
      "The crowd is mesmerized by Jack's words,\n",
      "As the sky turns from deep blue to emerald green.\n",
      "They feel the warmth of the sun on their skin,\n",
      "And they hear the chirping of the birds around.\n",
      "\n",
      "Suddenly, a gust of wind blows through the air,\n",
      "And the crowd gasps in disbelief.\n",
      "They see a brightly colored butterfly fluttering,\n",
      "And\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('/Users/jasper/Downloads/My_first_agents-main/notebooks/.env')\n",
    "\n",
    "# Verify that the API key is loaded\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    try:\n",
    "        response = inference_client.post(\n",
    "            json={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 150,  # Adjusted to limit the length of the response\n",
    "                    \"top_p\": 0.95,          # Adjusted for more diverse responses\n",
    "                    \"temperature\": 0.8      # Adjusted for more creative responses\n",
    "                },\n",
    "                \"task\": \"text-generation\",\n",
    "            },\n",
    "        )\n",
    "        response_json = json.loads(response.decode('utf-8'))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the repository ID for the model\n",
    "repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Initialize the InferenceClient with the API key\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    token=hf_api_key,  # Pass the API key\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Call the function with a prompt\n",
    "prompt = \"Hello! What is happiness?\"\n",
    "generated_text = call_llm(llm_client, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: (Request ID: rJ9AcFcK9BvY3J7CbA78c)\n",
      "\n",
      "Bad request:\n",
      "Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('/Users/jasper/Downloads/My_first_agents-main/notebooks/.env')\n",
    "\n",
    "# Verify that the API key is loaded\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    try:\n",
    "        response = inference_client.post(\n",
    "            json={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 150,  # Adjusted to limit the length of the response\n",
    "                    \"top_p\": 0.95,          # Adjusted for more diverse responses\n",
    "                    \"temperature\": 0.8      # Adjusted for more creative responses\n",
    "                },\n",
    "                \"task\": \"text-generation\",\n",
    "            },\n",
    "        )\n",
    "        response_json = json.loads(response.decode('utf-8'))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the repository ID for the model\n",
    "repo_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Initialize the InferenceClient with the API key\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    token=hf_api_key,  # Pass the API key\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Call the function with a prompt\n",
    "prompt = \"Hello! What is happiness?\"\n",
    "generated_text = call_llm(llm_client, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('/Users/jasper/Downloads/My_first_agents-main/notebooks/.env')\n",
    "\n",
    "# Get the Hugging Face API key from the environment variable\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "# Initialize the InferenceClient with the API key\n",
    "client = InferenceClient(api_key=hf_api_key)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of France?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2-VL-7B-Instruct\", \n",
    "    messages=messages, \n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How can you be happy in the face of adversity?\n",
      "\n",
      "Happiness is a complex and multifaceted emotion that can be influenced by a variety of factors such as genetics, environment, culture, and personal experiences. Some people may find happiness in specific activities or relationships, while others may experience it in moments of self-realization or spiritual connection.\n",
      "\n",
      "It is important to remember that happiness is not always an easy experience to achieve, especially when facing adversity or difficult situations. However, it is possible to cultivate happiness and resilience even in the face of challenges.\n",
      "\n",
      "Here are some tips to help you be happy in the face of adversity:\n",
      "\n",
      "1. Practice gratitude: Focus on the good things in your life and express gratitude for them.\n",
      "\n",
      "2. Build positive relationships: Surround\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('/Users/jasper/Downloads/My_first_agents-main/notebooks/.env')\n",
    "\n",
    "# Verify that the API key is loaded\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    try:\n",
    "        response = inference_client.post(\n",
    "            json={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 150,  # Adjusted to limit the length of the response\n",
    "                    \"top_p\": 0.95,          # Adjusted for more diverse responses\n",
    "                    \"temperature\": 0.8      # Adjusted for more creative responses\n",
    "                },\n",
    "                \"task\": \"text-generation\",\n",
    "            },\n",
    "        )\n",
    "        response_json = json.loads(response.decode('utf-8'))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the repository ID for the model\n",
    "repo_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "\n",
    "# Initialize the InferenceClient with the API key\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    token=hf_api_key,  # Pass the API key\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Call the function with a prompt\n",
    "prompt = \"Hello! What is happiness?\"\n",
    "generated_text = call_llm(llm_client, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
